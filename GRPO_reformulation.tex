\documentclass[11pt]{paperFormatting}

\copyrightyear{2026}


\begin{document}

\title{DeepSeek's \\GRPO Notation\\ \& Why It's Wrong}
\author{Jordan Jiosi}
\makeTitlePage
\buildAbstract

\newpage

\section{Group Relative Policy Optimization (GRPO)}
\label{sec:GRPO}

    Group Relative Policy Optimization (GRPO) is a group-normalized policy-gradient objective made popular by DeepSeek's 2024 paper \cite{shao2024deepseekmath}.
    The core idea is to replace a single-sample advantage estimate with a \textit{relative} advantage computed within a small group of candidate actions, or trajectories, conditioned on the same context, for example the same state or the same initial seeded state.
    This yields a variance-reduced learning signal while retaining the same fundamental policy-gradient structure used in trust-region style policy optimization, for example Proximal Policy Optimization (PPO)-style clipped objectives and KL-anchored updates.

    This analysis concerns the interpretation and notation of the GRPO objective rather than its empirical performance. The method itself is not claimed to be incorrect; rather, the focus is on how its regularization term is framed and justified relative to standard policy-gradient and trust-region formulations.

    Consider a context variable $s$ denoting a state.
    Let a policy $\pi_{\theta}(a\vert s)$ generate a group of $G$ candidate actions $\{a_i\}_{i=1}^{G}$, each receiving a scalar return or reward $r_i \approx r(s,a_i)$.
    Then, we define the group baseline as the mean
    $
        \bar{r}(s) \approx \frac{1}{G}\sum_{i=1}^{G} r(s,a_i)
    $
    and define the group-relative advantage
    $$
        \hat{A}_i \doteq r(s,a_i) - \bar{r}(s)
        \implies \qquad
        \tilde{A}_i \approx \frac{\hat{A}_i}{\sum_r(s) + \epsilon}
        \implies \qquad
        \sum_r(s) \approx \sqrt{\frac{1}{G}\sum_{i=1}^{G}\big(r(s,a_i)-\bar{r}(s)\big)^2}
    $$

    Next, we define a behavior policy $\pi_{\theta_{\mathrm{old}}}$ and a fixed reference policy $\pi_{\mathrm{ref}}$.

    As in standard importance-weighted policy-gradient updates, including TRPO/PPO-style implementations, the importance sampling ratio is taken with respect to the behavior policy:
    $$
        \rho_i(\theta) \approx \frac{\pi_{\theta}(a_i\vert s)}{\pi_{\theta_{\mathrm{old}}}(a_i\vert s)}
    $$

    This ratio plays the same mechanical role as in PPO; no novel estimator is introduced at this stage.

    Separately, GRPO introduces an additional reference-anchored log-ratio term, which is \textit{not} the PPO importance ratio:
    $$
        \ell_i(\theta) \approx \log \pi_{\theta}(a_i\vert s) - \log \pi_{\mathrm{ref}}(a_i\vert s)
    $$

    \subsection{KL Divergence, Trust Regions, and What GRPO Actually Optimizes}
    \label{subsec:kl_trust_region_context}

Policy-gradient methods are known to be unstable when policy updates are too large; even a single step that substantially alters the action distribution can degrade performance or collapse training. A common stabilization strategy is therefore to constrain successive policies to remain close in distribution, most often through a Kullback--Leibler (KL) divergence penalty or constraint.

The KL divergence between two distributions $P$ and $Q$ is defined as
\begin{equation}
    D_{\mathrm{KL}}(P\Vert Q)
    \doteq
    \EX_{x}\!\left[\log\left(\frac{P(x)}{Q(x)}\right)\right]
    \quad\backepsilon\quad
    x \sim P
    \label{eq:KL_divergence}
\end{equation}

Crucially, this quantity is only a KL divergence when the expectation is taken under the same distribution whose deviation is being measured. If the expectation is taken under a different distribution, the expression remains a log-ratio, but it no longer corresponds to a KL divergence in the information-theoretic sense.

For notational clarity, we define the conditional action distribution
\[
    \pi_{\theta}^s(a) \doteq \pi_{\theta}(a\vert s),
\]
so that KL divergences between policies conditioned on the same context may be written without ambiguity.

For policies conditioned on context $s$, this specialization takes the form
\begin{equation}
    D_{\mathrm{KL}}\!\left(\pi_{\theta}^s\,\Vert\,\pi_{\mathrm{ref}}^s\right)
    =
    \EX_{a\sim \pi_{\theta}^s}\!\left[
        \log\frac{\pi_{\theta}^s(a)}{\pi_{\mathrm{ref}}^s(a)}
        \right]
    \label{eq:kl_divergence_conditional}
\end{equation}

This distinction is not cosmetic. The KL divergence is an expectation over the *current* policy, and its interpretation depends entirely on that choice of measure.

\subsubsection{Trust-Region Methods}
\label{subsubsec:trust_region_methods}
In Trust-Region Policy Optimization (TRPO), this idea appears as a constraint or penalty that limits how far an updated policy may drift from a reference distribution. Abstractly, such updates under the rollout assume the form
\[
    \max_{\theta}\; \EX_{s,a\sim \pi_{\theta_{\mathrm{old}}}}\!\big[\rho(\theta)\,A(s,a)\big]
    \quad\text{s.t.}\quad
    \EX_{s}\!\left[D_{\mathrm{KL}}\!\left(\pi_{\theta}(a\vert s)\,\Vert\,\pi_{\theta_{\mathrm{old}}}(a\vert s)\right)\right] \le \delta
\]

In practice, PPO does not solve this constrained optimization problem directly. Instead, it approximates trust-region behavior through clipping and, optionally, by monitoring or penalizing the empirical KL divergence. The resulting update is heuristic but effective: stability arises from small policy steps rather than from exact enforcement of a KL constraint.

\subsubsection{What GRPO Inherits From PPO}
\label{subsubsec:GRPO_inherits_PPO}
GRPO introduces a reference-anchored log-ratio term
\[
    \ell_i(\theta) \doteq \log \pi_{\theta}(a_i\vert s) - \log \pi_{\mathrm{ref}}(a_i\vert s)
\]
which is a pointwise random variable. By itself, this quantity is not a KL divergence. It becomes one only after taking an expectation under the same policy that appears in the numerator.

\subsubsection{Where GRPO Breaks}
\label{subsubsec:GRPO_breaks}
GRPO evaluates this log-ratio under samples drawn from the behavior policy $\pi_{\theta_{\mathrm{old}}}$ rather than from the current policy $\pi_{\theta}$. When written explicitly as 
\[
    \EX_{s, a \vert \pi_{\theta_{\mathrm{old}}}}
    \left[\log\left(\frac{\pi_{\theta}(a\vert s)}{\pi_{\mathrm{ref}}(a\vert s)}\right)\right]
    \quad\backepsilon\quad
    s \sim d^{\pi_{\theta}},\; a \sim \pi_{\theta}(a\vert s)
\]

the result is that GRPO's regularization term is not a KL divergence in the strict sense. It is a pointwise log-ratio penalty evaluated under an off-policy distribution.

This mirrors the approximation already employed in PPO, where trust-region behavior arises from clipping or auxiliary KL monitoring rather than from optimizing a true KL-constrained objective. DeepSeekâ€™s formulation implicitly assumes this same identification, treating the off-policy expectation as if it were taken under the current policy, even though this assumption is never stated explicitly.

\subsubsection{GRPO's Objective Reformulated}
\label{subsubsec:GRPO_objective_reformulated}
Rewriting DeepSeek's GRPO objective to make its assumptions explicit, and placing it side-by-side with the PPO surrogate objective, one obtains:

\begin{equation}
    \mathcal{J}_{\mathrm{GRPO}}(\theta)
    \approx
    \EX_{s, a_{1:G}\vert \pi_{\theta_{\mathrm{old}}}}
    \left[
        \frac{1}{G}\sum_{i=1}^{G}
        \left(
            \rho_i(\theta)\,\tilde{A}_i
            -
            \beta\,\ell_i(\theta)
        \right)
    \right]
    \label{eq:grpo_obj}
\end{equation}

where $\theta_{\mathrm{old}}$ denotes the behavior policy used to sample the group, and $\beta>0$ is a regularization weight \cite{deepseek2025r1}.

If one introduces PPO-style clipping on the ratio, one obtains the clipped surrogate:

\begin{equation}
    \mathcal{J}_{\mathrm{GRPO\text{-}clip}}(\theta)
    \approx
    \EX_{s, a_{1:G}\vert \pi_{\theta_{\mathrm{old}}}}
    \left[
        \frac{1}{G}\sum_{i=1}^{G}
        \min\!\Big(\rho_i(\theta)\,\tilde{A}_i,\;
        \mathrm{clip}\big(\rho_i(\theta),1-\varepsilon,1+\varepsilon\big)\,\tilde{A}_i\Big)
        -
        \beta\,\ell_i(\theta)
    \right]
\end{equation}

recalling $\varepsilon>0$ is the PPO clipping parameter.

\subsubsection{PPO vs GRPO}
\label{subsubsec:PPO_vs_GRPO}
GRPO retains the core PPO mechanisms under the hood. The update still uses importance sampling ratios, a clipped surrogate objective function, and trust-region like behavior that keeps steps small in practice.

The substantive difference lies in GRPO removing the learned value function by replacing generalized advantage estimation style advantage estimation with group-normalized rewards, hence the nomenclature paradigm \textit{Group Relative Policy Optimization}.

The trade-off is theoretical clarity for lower variance and reduced resource use, which is a reasonable computational engineering choice, but it weakens the clean interpretation of the regularizer as a KL penalty when combined with off-policy sampling.

The empirical effectiveness of GRPO likely arises from small policy updates, comparative reward modeling, and the surrounding training pipeline rather than from the theoretical properties of the regularization term itself.

In short, GRPO inherits PPO's stability heuristics but not its theoretical KL interpretation.

\section{DeepSeek's Problematic GRPO Notation: A Novel Analysis}
\label{sec:GRPO_reformulation}

    \subsection{What Went Wrong}
    \label{sec:deepseek_grpo_intro}

    DeepSeek describes GRPO as a KL-regularized variant of PPO.
    While the algorithm is empirically effective, the formulation conflates several distinct quantities, leading to a misleading interpretation of the regularization term as written in the paper.

    \subsection{\texorpdfstring{Pointwise Log-Ratios $\not\doteq$ KL Divergences}{Pointwise Log-Ratios are not KL Divergences}}
    \label{sec:grpo_pointwise}

    This is the key distinction: a log-ratio evaluated at a single sample is not a divergence, but merely a scalar random variable.
    Recall the Kullback-Leibler divergence definition between two policies $\pi_\theta$ and $\pi_{\mathrm{ref}}$ in \eqref{eq:kl_divergence_conditional}: without the expectation under $\pi_\theta$, 
    the quantity $\log\!\left(\frac{\pi_\theta}{\pi_{\mathrm{ref}}}\right)$ is simply a log-likelihood ratio.
    DeepSeek's reference to a single-sample realization of this ratio as ``the KL'' is misleading at best and formally incorrect at worst.

    DeepSeek-R1 instead introduces the nonnegative function
    $$
    \frac{\pi_{\mathrm{ref}}(a\vert s)}{\pi_\theta(a\vert s)}
    -
    \log\!\left(
    \frac{\pi_{\mathrm{ref}}(a\vert s)}{\pi_\theta(a\vert s)}
    \right)
    -1
    $$
    and labels it an estimator of $D_{\mathrm{KL}}(\pi_\theta\|\pi_{\mathrm{ref}})$ \cite{deepseek2025r1}.

    This expression corresponds to a variational convex-analytic representation of the KL divergence and yields an unbiased estimator \textit{only} when the expectation is taken under the state-action visitation distribution induced by $\pi_\theta$.
    That is to say, the KL divergence only holds under expectation with respect to the target distribution.

    \subsection{Off-Policy Sampling Breaks DeepSeek's Unbiasedness Claim}
    \label{sec:grpo_offpolicy}

    In GRPO, the expectation is taken under $\EX_{s, a \sim \pi_{\theta_{\mathrm{old}}}}$ rather than $\EX_{s, a \sim \pi_{\theta}}$ such that the KL surrogate is inserted into the objective without importance weighting.

    Consequently, three critical issues arise.

    First, the surrogate is not, in general, an unbiased estimator of $D_{\mathrm{KL}}(\pi_\theta\|\pi_{\mathrm{ref}})$ because the expectation is taken under $\pi_{\theta_{\mathrm{old}}}$ rather than $\pi_\theta$.

    Second, the usual control-variate interpretation does not strictly apply because the expectation is taken under a different measure than the one defining the KL; this does not make the method incorrect, but it does invalidate a literal KL interpretation.

    Finally, even though the surrogate is nonnegative, that property is orthogonal to whether it estimates the claimed KL divergence.

    \subsection{Practical Implications}
    \label{sec:grpo_practical}

    Practically, the regularization term behaves more like an entropy-style constraint. The reference policy $\pi_{\mathrm{ref}}$ is treated as fixed and does not contribute gradients directly; instead, it shapes the gradient of $\log \pi_\theta$ so as to discourage large deviations from the reference distribution.

    This is not inherently incorrect; it is a common and effective heuristic. However, it is not equivalent to optimizing a well-defined KL-regularized reinforcement learning objective under off-policy sampling as defined in the DeepSeek-R1 paper; presenting it as such conflates a heuristic regularization penalty with a KL-regularized objective.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\buildBibliography

\end{document}
